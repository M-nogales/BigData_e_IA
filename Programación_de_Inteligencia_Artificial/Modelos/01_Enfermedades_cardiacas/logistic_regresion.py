from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, recall_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import pandas as pd

# Load the dataset and check the first rows and some info
health = pd.read_csv('./Dataset_Enfermedades.csv')
health.info()
print(health.head())

# Features is the input data, the columns that will be used to make predictions.
# Target variable is the output data, the column that will be predicted.
# Features:
X = health[['edad','sexo','presion_sistolica','presion_diastolica','colesterol','glucosa','indice_masa_corporal','actividad_fisica','fumar','historia_familiar','diabetes']]
# Target variable:
y = health['enfermedad_cardiaca']

# Split data into training and test sets
# 20% of the data will be used for testing, 80% for training
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Data normalization
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train the logistic regression model
model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)
y_pred_prob = model.predict_proba(X_test)[:, 1]

'''
predict(X_test): performs predicctions about the test data.
y_pred_prob = predict_proba(X_test)[:, 1]: Probability that the test data belongs to the positive class.
in conclusion, the probability to have a heart disease.
'''
'''
example of return value of predict_proba(X_test):
each row corresponds to a sample in X_test, and each column corresponds to a class.
array([
    [0.8, 0.2],  # Sample 1 â†’ 80% chance of class 0, 20% chance of class 1
    [0.3, 0.7],  # Sample 2 â†’ 30% chance of class 0, 70% chance of class 1
    [0.1, 0.9],  # Sample 3 â†’ 10% chance of class 0, 90% chance of class 1
    ...
])
[:,1] returns the second column of the array, which corresponds to the probability of class 1.
'''

print("confussion matrix:")
print(confusion_matrix(y_test, y_pred))

accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc_roc = roc_auc_score(y_test, y_pred_prob)

# Results
print(f"ðŸ”¹ Accuracy: {accuracy:.2f}")
print(f"ðŸ”¹ Recall: {recall:.2f}")
print(f"ðŸ”¹ F1-Score: {f1:.2f}")
print(f"ðŸ”¹ AUC-ROC: {auc_roc:.2f}")

# Report
target_names = ['Prob no desease', 'Prob desease']
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=target_names))

# Compute the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', linewidth=2, label=f'AUC = {auc_roc:.2f}')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line (random model)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('ROC Curve - Logistic Regression')
plt.legend(loc='lower right')
plt.grid()
plt.show()

# Metrics comparation
def metrics_result_comparation(accuracy, recall, f1, auc_roc, model):
    metrics = ['Accuracy', 'Recall', 'F1-Score', 'AUC-ROC']
    values = [accuracy, recall, f1, auc_roc]
    
    # Create a bar chart
    plt.figure(figsize=(8, 6))
    plt.bar(metrics, values, color=['blue', 'green', 'orange', 'red'])
    
    # Add value labels on top of bars
    for i, v in enumerate(values):
        plt.text(i, v + 0.02, f"{v:.2f}", ha='center', fontsize=12, fontweight='bold')
    
    # Labels and title
    plt.ylim(0, 1.1)
    plt.ylabel('Score')
    plt.title('Model Performance Metrics - '+ model)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    
    # Show the plot
    plt.show()

metrics_result_comparation(accuracy, recall, f1, auc_roc, model = 'Logistic Regression')