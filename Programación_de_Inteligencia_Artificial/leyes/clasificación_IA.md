# Informe: Clasificación de Sistemas de IA según el Reglamento (UE) 2024/1689

## Introducción

Este informe tiene como objetivo aplicar el Reglamento de la Unión Europea 2024/1689 sobre Inteligencia Artificial (Reglamento de la IA), centrándose en los artículos 5 a 7, para clasificar distintos ejemplos de sistemas de IA en función del nivel de riesgo que representan.

---

## Estudio Normativo

### Artículo 5 – Prácticas Prohibidas

El artículo 5 establece categorías de usos de IA expresamente prohibidos, como:

* Manipulación subliminal (art. 5.1.a)
* Explotación de vulnerabilidades (art. 5.1.b)
* Puntuación social (art. 5.1.c)
* Evaluación predictiva de delitos (art. 5.1.d)
* Extracción masiva para reconocimiento facial (art. 5.1.e)
* Inferencia de emociones en el trabajo o escuelas (art. 5.1.f)
* Clasificación biométrica sensible (art. 5.1.g)
* Identificación biométrica remota en espacios públicos (art. 5.1.h)

### Artículo 6 – Sistemas de Alto Riesgo

Se consideran de **alto riesgo**:

* Sistemas que son componentes de seguridad de productos regulados por legislación armonizada de la UE (Anexo I).
* Sistemas descritos en el **Anexo III**, a menos que cumplan condiciones atenuantes del art. 6.3.
* Siempre que realicen **elaboración de perfiles de personas físicas**, serán de alto riesgo.

---

## Casos Reales Analizados

| Caso                                                                    | Tipo de Sistema                                                | Clasificación de Riesgo                                      | Justificación Normativa                                                                                       |
| ----------------------------------------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------- |
| Chatbot emocional en atención al cliente                                | IA conversacional con análisis de tono/emoción                 | Riesgo limitado                                              | No es de alto riesgo si no influye en decisiones automatizadas ni recoge biometría (Art. 6.3.a/b)             |
| Chatbot manipulador para ventas agresivas                               | IA conversacional que induce decisiones con técnicas engañosas | Prohibido                                                    | Art. 5.1.a: manipulación deliberada con técnicas subliminales                                                 |
| Reconocimiento facial en un aeropuerto                                  | Identificación biométrica remota en espacio público            | Alto riesgo (o prohibido según el uso)                       | Art. 5.1.h: solo permitido bajo estrictas condiciones de seguridad pública                                    |
| Scoring crediticio automatizado                                         | IA para evaluar solvencia financiera de personas físicas       | Alto riesgo                                                  | Anexo III, punto 5 + Art. 6.2: afecta derechos fundamentales                                                  |
| Generación de deepfakes con IA                                          | IA para crear vídeos realistas falsos de personas              | Riesgo limitado (o alto si se usa con fines de manipulación) | Si se usa para desinformar o manipular, puede estar prohibido por art. 5.1.a; si no, sería de riesgo limitado |
| IA para detectar emociones en empleados                                 | IA que monitoriza emociones en el trabajo                      | Prohibido                                                    | Art. 5.1.f: inferencia emocional en lugares de trabajo está prohibida salvo fines médicos o de seguridad      |
| IA para puntuación social en escuelas                                   | Evaluación de alumnos por comportamiento general               | Prohibido                                                    | Art. 5.1.c: puntuación social injustificada o desproporcionada                                                |
| Sistema de IA para predecir reincidencia delictiva sin hechos objetivos | Sistema predictivo basado en perfiles psicológicos             | Prohibido                                                    | Art. 5.1.d: evaluación de riesgo de delitos sin hechos objetivos                                              |
| IA en selección de personal basada en CV + perfiles online              | Clasificación automatizada de candidatos laborales             | Alto riesgo                                                  | Anexo III, punto 4 + Art. 6.2: afecta acceso al empleo                                                        |

---

## Argumentación de Clasificación

### 1. **Chatbot emocional**

Aunque puede analizar emociones, su uso como asistente comercial no lo convierte automáticamente en alto riesgo si no influye en decisiones importantes ni afecta derechos. Si solo guía conversaciones, su impacto es limitado (Art. 6.3.b).

### 2. **Chatbot manipulador**

Cuando un chatbot influye deliberadamente en decisiones del usuario usando técnicas subliminales o engañosas, incurre en una práctica prohibida (Art. 5.1.a).

### 3. **Reconocimiento facial en aeropuerto**

Se encuentra entre los usos regulados estrictamente (Art. 5.1.h). Es alto riesgo y solo permitido bajo autorización judicial y en situaciones de seguridad claramente justificadas.

### 4. **Scoring crediticio**

Clasificado como alto riesgo por el impacto directo en derechos económicos (Anexo III, punto 5). Tiene consecuencias sobre el acceso a crédito, bienes y servicios.

### 5. **Deepfakes**

En general, no se considera alto riesgo si se utiliza para entretenimiento o arte. Pero si se emplea para manipulación política, puede estar prohibido (Art. 5.1.a o 5.1.c según el caso).

### 6. **Detección emocional en el trabajo**

Este uso está expresamente **prohibido**, salvo motivos médicos o de seguridad (Art. 5.1.f).

### 7. **Puntuación social escolar**

Prohibida como forma de evaluar personas fuera de contexto o de manera desproporcionada (Art. 5.1.c).

### 8. **Predicción de delitos**

Totalmente prohibido si se basa en perfiles psicológicos sin hechos concretos (Art. 5.1.d).

### 9. **IA para selección de personal**

Se clasifica como de alto riesgo (Anexo III, punto 4) porque influye en el acceso al empleo, un derecho fundamental.

---

## Conclusión

El Reglamento (UE) 2024/1689 establece claramente límites y clasificaciones para el uso de la IA. En función del contexto y finalidad, los sistemas analizados abarcan desde riesgo limitado hasta prácticas completamente prohibidas. La correcta clasificación ayuda a garantizar la protección de los derechos fundamentales de las personas en entornos digitales y automatizados.